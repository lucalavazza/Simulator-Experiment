{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC$\\pi$BO - Simulator-based Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "## Common libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "import pickle\n",
    "from itertools import cycle, chain\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "## Plotting\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.pyplot import rc\n",
    "\n",
    "## Graphs\n",
    "from networkx.drawing import nx_agraph\n",
    "import networkx as nx\n",
    "from graphviz import Source\n",
    "import pygraphviz\n",
    "\n",
    "## GPs\n",
    "from GPy.kern import RBF\n",
    "from GPy.models.gp_regression import GPRegression\n",
    "\n",
    "## DCBO Utils\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from src.utils.utilities import powerset\n",
    "from src.utils.sem_utils.sem_estimate import build_sem_hat\n",
    "from src.utils.sequential_intervention_functions import get_interventional_grids\n",
    "from src.experimental.experiments import optimal_sequence_of_interventions, run_methods_replicates\n",
    "from src.experimental.analyse_results import get_relevant_results, elaborate, gap_metric_standard, get_common_initial_values, get_converge_trial\n",
    "from src.utils.plotting import plot_expected_opt_curve_paper\n",
    "\n",
    "## Optimisation Algos\n",
    "from src.methods.dcbo import DCBO\n",
    "from src.methods.bo import BO\n",
    "from src.methods.pibo import PIBO\n",
    "from src.methods.dcpibo import DCPIBO\n",
    "\n",
    "## YT\n",
    "import random\n",
    "from gym import spaces\n",
    "from numpy import repeat\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "sys.path.append(\"../Yawning_Titan\")\n",
    "from yawning_titan.integrations.dcbo.dcbo_agent import DCBOAgent\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.helpers import network_creator\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_list(input_list, exponent):\n",
    "    '''Raises each element of the given input_list to the desired exponent'''\n",
    "    \n",
    "    return_list = []\n",
    "    \n",
    "    for element in input_list:\n",
    "        if element >= 0:\n",
    "            raised_element = element**exponent\n",
    "        else:\n",
    "            raised_element = -(abs(element)**exponent)\n",
    "        return_list.append(raised_element)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(y_function, standard_deviation):\n",
    "    '''Computes all the necessary lists associated to the Normal Distribution'''\n",
    "    \n",
    "    return_variates = []\n",
    "    return_amps = []\n",
    "    return_pdfs = []\n",
    "    \n",
    "    for mean in y_function:\n",
    "        return_variates.append(stats.norm.rvs(mean, standard_deviation, 10))\n",
    "        \n",
    "        amp = np.linspace(mean-5*standard_deviation, mean+5*standard_deviation, 10)\n",
    "        return_amps.append(amp)\n",
    "        \n",
    "        return_pdfs.append(stats.norm.pdf(amp, mean, standard_deviation))\n",
    "    \n",
    "    return return_variates, return_amps, return_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_pdfs(pdfs_list):\n",
    "    '''Normalises the PDFs between 0 and 1'''\n",
    "    \n",
    "    return_normalised_pdfs_list = []\n",
    "    \n",
    "    for pdf_list in pdfs_list:\n",
    "        temp_list = []\n",
    "        \n",
    "        pdf_min = min(pdf_list)\n",
    "        pdf_max = max(pdf_list)\n",
    "        \n",
    "        for pdf_value in pdf_list:\n",
    "            temp_list.append(round((pdf_value-pdf_min)/(pdf_max-pdf_min),2))\n",
    "        \n",
    "        return_normalised_pdfs_list.append(temp_list)\n",
    "        \n",
    "    return return_normalised_pdfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret_priors(start_prior):\n",
    "    '''Adds regret'''\n",
    "    \n",
    "    return_raised_priors = []\n",
    "    \n",
    "    for i in range(1,N+1):\n",
    "        temp = []\n",
    "        gamma = beta/i\n",
    "        \n",
    "        for p_list in start_prior:\n",
    "            temp.append(power_list(p_list, gamma))\n",
    "            \n",
    "        return_raised_priors.append(temp)\n",
    "    \n",
    "    return return_raised_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_optima_regret(iterations, time_steps, regret_priors, normalised_pdfs):\n",
    "    '''Computes the prediction for each time-step and each iteration, according to the effect of gamma (regret)'''\n",
    "    \n",
    "    return_predictions = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        temp = []\n",
    "        \n",
    "        for time_step in range(time_steps):\n",
    "            if(min(regret_priors[iteration][time_step])+max(regret_priors[iteration][time_step]))<0:\n",
    "                optimum = min(np.multiply(regret_priors[iteration][time_step], normalised_pdfs[time_step]))\n",
    "            else:\n",
    "                optimum = max(np.multiply(regret_priors[iteration][time_step], normalised_pdfs[time_step]))\n",
    "                              \n",
    "            temp.append(optimum)\n",
    "                              \n",
    "        return_predictions.append(temp)\n",
    "                              \n",
    "    return return_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "* Use YAWNING TITAN to simulate data for a range of probabilities for $P$ and $I$\n",
    "* Generate data for 25 timesteps in each environment\n",
    "* Use BO, CBO and DCBO to take that data and determine the optimal value for $P$ and $I$ in the final 3 timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DAG Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picshat_graph(start_time: int, stop_time: int, verbose=False):\n",
    "    \"\"\"\n",
    "    Helper function to construct our DAG as a NetworkX graph.\n",
    "    \"\"\"\n",
    "    assert start_time <= stop_time\n",
    "\n",
    "    spatial_edges, ranking = [], []\n",
    "    nodes = [\"P\", \"I\", \"H\", \"A\", \"C\", \"T\", \"S\"]\n",
    "    node_count = len(nodes)\n",
    "    connections = node_count * \"{}_{} -> {}_{}; \"\n",
    "    edge_pairs = [\"P\", \"A\", \"P\", \"H\", \"I\", \"A\", \"I\", \"S\", \"C\", \"H\", \"C\", \"T\", \"A\", \"T\"]\n",
    "    pair_count = len(edge_pairs)\n",
    "    for t in range(start_time, stop_time + 1):\n",
    "        space_idx = pair_count * [t]\n",
    "        iters = [iter(edge_pairs), iter(space_idx)]\n",
    "        inserts = list(chain(map(next, cycle(iters)), *iters))\n",
    "        spatial_edges.append(connections.format(*inserts))\n",
    "        ranking.append(\n",
    "            \"{{ rank=same; {} }} \".format(\n",
    "                \" \".join([item + \"_{}\".format(t) for item in nodes])\n",
    "            )\n",
    "        )\n",
    "    ranking = \"\".join(ranking)\n",
    "    spatial_edges = \"\".join(spatial_edges)\n",
    "\n",
    "    temporal_edges = []\n",
    "    NUMBER_TRANS_EDGES = 2\n",
    "    connections = NUMBER_TRANS_EDGES * \"{}_{} -> {}_{}; \"\n",
    "    for t in range(stop_time):\n",
    "        edge_pairs = [\"C\", \"C\", \"S\", \"S\"]\n",
    "        temporal_idx = node_count * [t, t + 1]\n",
    "        iters = [iter(edge_pairs), iter(temporal_idx)]\n",
    "        inserts = list(chain(map(next, cycle(iters)), *iters))\n",
    "        temporal_edges.append(connections.format(*inserts))\n",
    "    temporal_edges.append(\"\")\n",
    "    temporal_edges = \"\".join(temporal_edges)\n",
    "    graph = \"digraph {{ rankdir=LR; {} {} {} }}\".format(\n",
    "        spatial_edges, temporal_edges, ranking\n",
    "    )\n",
    "    if verbose:\n",
    "        return Source(graph)\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_view = picshat_graph(0, T - 1, verbose=True)\n",
    "dag = nx_agraph.from_agraph(pygraphviz.AGraph(dag_view.source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Generation \n",
    "The following cell uses YAWNING TITAN to set up 10 different environments, each with a random value for $P$ and $I$, and runs them for 25 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(use_same_net=False):\n",
    "    \"\"\"\n",
    "    Helper function to create an environment.\n",
    "\n",
    "    Args:\n",
    "        use_same_net: If true uses a saved network, otherwise creates a new network\n",
    "\n",
    "    Returns: The env\n",
    "\n",
    "    \"\"\"\n",
    "    settings_path = \"dcbo_config.yaml\"\n",
    "\n",
    "    if use_same_net:\n",
    "        matrix, node_positions = network_creator.load_network(\"dcbo_base_net.txt\")\n",
    "    else:\n",
    "        matrix, node_positions = network_creator.create_mesh(size=10)\n",
    "\n",
    "    network_interface = NetworkInterface(\n",
    "        matrix, node_positions, settings_path=settings_path\n",
    "    )\n",
    "\n",
    "    red = RedInterface(network_interface)\n",
    "    blue = BlueInterface(network_interface)\n",
    "\n",
    "    number_of_actions = blue.get_number_of_actions()\n",
    "\n",
    "    env = GenericNetworkEnv(\n",
    "        red,\n",
    "        blue,\n",
    "        network_interface,\n",
    "        number_of_actions,\n",
    "        show_metrics_every=10,\n",
    "        collect_additional_per_ts_data=True,\n",
    "    )\n",
    "\n",
    "    # check_env(env, warn=True)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    return env\n",
    "\n",
    "# Creates all 10 envs\n",
    "all_envs = [create_env() for i in range(10)]\n",
    "agent = DCBOAgent(all_envs[0].action_space, [0.5, 0.5])\n",
    "\n",
    "TIMESTEPS = 25\n",
    "COSTS = {\"restore_node\": 1, \"isolate\": 1, \"do_nothing\": 0, \"compromise\": 10}\n",
    "\n",
    "centralities = nx.degree_centrality(all_envs[0].network_interface.current_graph)\n",
    "slice_data = {k: [] for k in \"PICSHAT\"}\n",
    "\n",
    "for counter, current_env in enumerate(all_envs):\n",
    "    # Init data\n",
    "    slice_p, slice_i, slice_s, slice_h, slice_c, slice_a, slice_t = (list() for i in range(7))\n",
    "    \n",
    "    # Init probabilities\n",
    "    current_env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    w1, w2 = (\n",
    "        random.gauss(0.5, 0.167),\n",
    "        random.gauss(0.5, 0.167),\n",
    "    )\n",
    "    agent.update_probabilities([w1, w2])\n",
    "\n",
    "    for i in range(TIMESTEPS):\n",
    "        done = False\n",
    "        current_step = 0\n",
    "\n",
    "        action = agent.predict(\"\", \"\", \"\", current_env)\n",
    "        env_observation, reward, done, notes = current_env.step(action)\n",
    "\n",
    "        c_cost = (COSTS[\"compromise\"] * sum(notes[\"end_state\"].values())) ** 1.5\n",
    "        action_cost = COSTS.get(notes[\"blue_action\"], 0)\n",
    "        surface = sum(\n",
    "            map(\n",
    "                lambda x: not x[0] and not x[1],\n",
    "                zip(\n",
    "                    notes[\"end_state\"].values(),\n",
    "                    current_env.network_interface.get_all_isolation().values(),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        # Append P, I and S\n",
    "        slice_p.append(agent.probabilities[0])\n",
    "        slice_i.append(agent.probabilities[1])\n",
    "        slice_s.append(surface) \n",
    "        \n",
    "        # Calculate and append H\n",
    "        h = 0\n",
    "        node_states = current_env.network_interface.get_all_node_compromised_states()\n",
    "        node_iso = current_env.network_interface.get_all_isolation()\n",
    "        comp_nodes = [k for k in node_states if node_states[k] == 1]\n",
    "        isolated_nodes = [k for k in node_iso if node_iso[k]]\n",
    "        for node in comp_nodes:\n",
    "            conn_nodes = current_env.network_interface.get_current_connected_nodes(node)\n",
    "            for c in conn_nodes:\n",
    "                if c in isolated_nodes:\n",
    "                    continue\n",
    "                h += current_env.network_interface.get_single_node_vulnerability(c)\n",
    "        slice_h.append(h)\n",
    "        # Append C, A and T\n",
    "        slice_c.append(c_cost)\n",
    "        slice_a.append(action_cost)\n",
    "        slice_t.append(action_cost + c_cost)\n",
    "\n",
    "    slice_data[\"P\"].append(np.asarray(slice_p))\n",
    "    slice_data[\"I\"].append(np.asarray(slice_i))\n",
    "    slice_data[\"S\"].append(np.asarray(slice_s))\n",
    "    slice_data[\"H\"].append(np.asarray(slice_h))\n",
    "    slice_data[\"C\"].append(np.asarray(slice_c))\n",
    "    slice_data[\"A\"].append(np.asarray(slice_a))\n",
    "    slice_data[\"T\"].append(np.asarray(slice_t))\n",
    "\n",
    "for k in slice_data.keys():\n",
    "    slice_data[k] = np.asarray(slice_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gp(\n",
    "    x,\n",
    "    y,\n",
    "    lengthscale=1.0,\n",
    "    variance=1.0,\n",
    "    noise_var=1.0,\n",
    "    ard=False,\n",
    "    n_restart=10,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to fit a GP\n",
    "    \"\"\"\n",
    "    kernel = RBF(x.shape[1], ARD=ard, lengthscale=lengthscale, variance=variance)\n",
    "    model = GPRegression(X=x, Y=y, kernel=kernel, noise_var=noise_var)\n",
    "    model.optimize_restarts(n_restart, verbose=False, robust=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vector = np.tile(\n",
    "    np.linspace(0, slice_data[\"P\"].shape[1] - 1, slice_data[\"P\"].shape[1]),\n",
    "    10\n",
    ")[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our GPs for t=0 (so no inter-timeslice dependencies)\n",
    "model_P = fit_gp(time_vector, np.hstack(slice_data[\"P\"])[:, np.newaxis])\n",
    "model_I = fit_gp(time_vector, np.hstack(slice_data[\"I\"])[:, np.newaxis])\n",
    "model_S = fit_gp(\n",
    "    np.hstack(slice_data[\"I\"])[:, np.newaxis], np.hstack(slice_data[\"S\"])[:, np.newaxis]\n",
    ")\n",
    "model_C = fit_gp(time_vector, np.hstack(slice_data[\"C\"])[:, np.newaxis])\n",
    "model_H = fit_gp(\n",
    "    np.hstack(\n",
    "        (\n",
    "            np.hstack(slice_data[\"P\"])[:, np.newaxis],\n",
    "            np.hstack(slice_data[\"C\"])[:, np.newaxis],\n",
    "        )\n",
    "    ), \n",
    "    np.hstack(slice_data[\"H\"])[:, np.newaxis],\n",
    ")\n",
    "model_A = fit_gp(\n",
    "    np.hstack(\n",
    "        (\n",
    "            np.hstack(slice_data[\"P\"])[:, np.newaxis],\n",
    "            np.hstack(slice_data[\"I\"])[:, np.newaxis],\n",
    "        )\n",
    "    ),\n",
    "    np.hstack(slice_data[\"A\"])[:, np.newaxis],\n",
    ")\n",
    "model_T = fit_gp(\n",
    "    np.hstack(\n",
    "        (\n",
    "            np.hstack(slice_data[\"C\"])[:, np.newaxis],\n",
    "            np.hstack(slice_data[\"A\"])[:, np.newaxis],\n",
    "        )\n",
    "    ),\n",
    "    np.hstack(slice_data[\"T\"])[:, np.newaxis],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_0 = {\n",
    "    \"P\": model_P,\n",
    "    \"I\": model_I,\n",
    "    \"S\": model_S,\n",
    "    \"H\": model_H,\n",
    "    \"C\": model_C,\n",
    "    \"A\": model_A,\n",
    "    \"T\": model_T,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting slice_data in the right form, based on the DAG\n",
    "S_AR, H_AR, C_AR, A_AR, T_AR = [], [], [], [], []\n",
    "\n",
    "for i in range(slice_data[\"P\"].shape[0]):\n",
    "    S_AR.append(\n",
    "        np.transpose(np.vstack((slice_data[\"C\"][i][:-1],slice_data[\"I\"][i][1:],slice_data[\"S\"][i][1:])))\n",
    "    )\n",
    "    C_AR.append(\n",
    "        np.transpose(np.vstack((slice_data[\"H\"][i][:-1],slice_data[\"C\"][i][1:])))\n",
    "    )\n",
    "    H_AR.append(\n",
    "        np.transpose(np.vstack((slice_data[\"P\"][i][1:],slice_data[\"C\"][i][1:],slice_data[\"H\"][i][1:])))\n",
    "    )\n",
    "    A_AR.append(\n",
    "        np.transpose(np.vstack((slice_data[\"P\"][i][1:],slice_data[\"I\"][i][1:],slice_data[\"A\"][i][1:])))\n",
    "    )\n",
    "    T_AR.append(\n",
    "        np.transpose(np.vstack((slice_data[\"C\"][i][1:],slice_data[\"A\"][i][1:],slice_data[\"T\"][i][1:])))\n",
    "    )\n",
    "\n",
    "S_AR = np.vstack(S_AR)\n",
    "H_AR = np.vstack(H_AR)\n",
    "C_AR = np.vstack(C_AR)\n",
    "A_AR = np.vstack(A_AR)\n",
    "T_AR = np.vstack(T_AR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_S_t = fit_gp(S_AR[:, :-1], S_AR[:, -1][:, np.newaxis])\n",
    "model_H_t = fit_gp(H_AR[:, :-1], H_AR[:, -1][:, np.newaxis])\n",
    "model_C_t = fit_gp(C_AR[:, :-1], C_AR[:, -1][:, np.newaxis])\n",
    "model_A_t = fit_gp(A_AR[:, :-1], A_AR[:, -1][:, np.newaxis])\n",
    "model_T_t = fit_gp(T_AR[:, :-1], T_AR[:, -1][:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_t = {\n",
    "    \"P\": model_P,\n",
    "    \"I\": model_I,\n",
    "    \"S\": model_S_t,\n",
    "    \"H\": model_H_t,\n",
    "    \"C\": model_C_t,\n",
    "    \"A\": model_A_t,\n",
    "    \"T\": model_T_t,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PICSHAT_SEM:\n",
    "    def __init__(self, functions_0, functions_t):\n",
    "        self.functions_0 = functions_0\n",
    "        self.functions_t = functions_t\n",
    "\n",
    "    def static(self):\n",
    "\n",
    "        P = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"P\"].predict(time * np.ones((1, 1)))[0]\n",
    "        )\n",
    "\n",
    "        I = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"I\"].predict(time * np.ones((1, 1)))[0]\n",
    "        )\n",
    "\n",
    "        S = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"S\"].predict(\n",
    "                np.transpose(np.vstack((sample[\"I\"][time] * np.ones((1, 1)))))\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        C = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"C\"].predict(time * np.ones((1, 1)))[0]\n",
    "        )\n",
    "\n",
    "        H = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"H\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"P\"][time] * np.ones((1, 1)),\n",
    "                            sample[\"C\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        A = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_0[\"A\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"P\"][time] * np.ones((1, 1)),\n",
    "                            sample[\"I\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        T = (\n",
    "            lambda noise, time, sample: np.abs(\n",
    "                noise\n",
    "                + self.functions_0[\"T\"].predict(\n",
    "                    np.transpose(\n",
    "                        np.vstack(\n",
    "                            (\n",
    "                                sample[\"C\"][time] * np.ones((1, 1)),\n",
    "                                sample[\"A\"][time] * np.ones((1, 1)),\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )[0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return OrderedDict(\n",
    "            [(\"P\", P), (\"I\", I), (\"S\", S), (\"H\", H), (\"C\", C), (\"A\", A), (\"T\", T)]\n",
    "        )\n",
    "\n",
    "    def dynamic(self):\n",
    "\n",
    "        P = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"P\"].predict(time * np.ones((1, 1)))[0]\n",
    "        )\n",
    "\n",
    "        I = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"I\"].predict(time * np.ones((1, 1)))[0]\n",
    "        )\n",
    "\n",
    "        S = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"S\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"C\"][time - 1] * np.ones((1, 1)),\n",
    "                            sample[\"I\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        C = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"C\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"H\"][time - 1] * np.ones((1,1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        H = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"H\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"P\"][time] * np.ones((1, 1)),\n",
    "                            sample[\"C\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        A = (\n",
    "            lambda noise, time, sample: noise\n",
    "            + self.functions_t[\"A\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"P\"][time] * np.ones((1, 1)),\n",
    "                            sample[\"I\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        T = lambda noise, time, sample: np.abs(\n",
    "            noise\n",
    "            + self.functions_t[\"T\"].predict(\n",
    "                np.transpose(\n",
    "                    np.vstack(\n",
    "                        (\n",
    "                            sample[\"C\"][time] * np.ones((1, 1)),\n",
    "                            sample[\"A\"][time] * np.ones((1, 1)),\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "        return OrderedDict(\n",
    "            [(\"P\", P), (\"I\", I), (\"S\", S), (\"H\", H), (\"C\", C), (\"A\", A), (\"T\", T)]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_domain = {\"P\": [0.0, 1.0], \"I\": [0.0, 1.0]}\n",
    "exploration_sets = list(powerset([\"P\", \"I\"]))\n",
    "n_to_compute = 50\n",
    "interventional_grids = get_interventional_grids(\n",
    "    exploration_sets, intervention_domain, size_intervention_grid=n_to_compute\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM = PICSHAT_SEM(functions_0, functions_t)\n",
    "initial_structural_equation_model = SEM.static()\n",
    "structural_equation_model = SEM.dynamic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    best_s_values,\n",
    "    best_s_sequence,\n",
    "    best_objective_values,\n",
    "    y_stars_all,\n",
    "    optimal_interventions,\n",
    "    all_CE,\n",
    ") = optimal_sequence_of_interventions(\n",
    "    exploration_sets,\n",
    "    interventional_grids,\n",
    "    initial_structural_equation_model,\n",
    "    structural_equation_model,\n",
    "    dag,\n",
    "    T=T,\n",
    "    model_variables=list(\"PISHCAT\"),\n",
    "    target_variable=\"T\",\n",
    "    task=\"min\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best intervention set:\", best_s_sequence)\n",
    "print(\"Best intervention values:\", best_s_values)\n",
    "print(\"Best objective values:\", best_objective_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models' executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis = np.linspace(1,T,T,dtype=int)\n",
    "std = 0.1\n",
    "beta = 1\n",
    "R = 3\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_obs_data = deepcopy(slice_data)\n",
    "for var in subset_obs_data.keys():\n",
    "    subset_obs_data[var] = subset_obs_data[var][:, -3:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_param = 0\n",
    "b_param = best_objective_values[1]\n",
    "y = a_param*t_axis+b_param # the line on which I assume the optima would be\n",
    "# y = np.zeros(len(t_axis))\n",
    "# y[0] = best_objective_values[0]\n",
    "# y[1:len(t_axis)] = best_objective_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variates, all_amps, all_pdfs = compute_statistics(y, std)\n",
    "all_variates_regret = regret_priors(all_variates)\n",
    "\n",
    "all_pdfs_normalised = normalise_pdfs(all_pdfs)\n",
    "\n",
    "predicted_optima_regret = predict_optima_regret(N, T, all_variates_regret, all_pdfs_normalised) # this are all the 𝜋s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See method for argument details\n",
    "good_results = run_methods_replicates(\n",
    "    G=dag,\n",
    "    sem=PICSHAT_SEM,\n",
    "    make_sem_estimator=build_sem_hat,\n",
    "    base_target_variable='T',\n",
    "    intervention_domain = intervention_domain,\n",
    "    methods_list = ['BO', 'PIBO', 'DCBO', 'DCPIBO'],\n",
    "    obs_samples = subset_obs_data,\n",
    "    exploration_sets = exploration_sets,\n",
    "    priors_regret = predicted_optima_regret,\n",
    "    total_timesteps = T,\n",
    "    number_of_trials = N,\n",
    "    reps = R, # Number of replicates (how many times we run each method)\n",
    "    n_restart = 1,\n",
    "    save_data = False,\n",
    "    n_obs = 5, # The method samples 5 time-series for each replicate\n",
    "    num_anchor_points = 100,\n",
    "    sample_anchor_points = True,\n",
    "    controlled_experiment=False,\n",
    "    args_sem=[functions_0, functions_t],\n",
    "    manipulative_variables=[\"P\", \"I\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_a_param = -3\n",
    "bad_y_param = -2\n",
    "bad_y = bad_a_param*t_axis+bad_y_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_all_variates, bad_all_amps, bad_all_pdfs = compute_statistics(bad_y, std)\n",
    "bad_all_variates_regret = regret_priors(bad_all_variates)\n",
    "\n",
    "bad_all_pdfs_normalised = normalise_pdfs(bad_all_pdfs)\n",
    "bad_predicted_optima_regret = predict_optima_regret(N, T, bad_all_variates_regret, bad_all_pdfs_normalised) # this are all the 𝜋s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_results = run_methods_replicates(\n",
    "    G=dag,\n",
    "    sem=PICSHAT_SEM,\n",
    "    make_sem_estimator=build_sem_hat,\n",
    "    base_target_variable='T',\n",
    "    intervention_domain = intervention_domain,\n",
    "    methods_list = ['BO', 'PIBO', 'DCBO', 'DCPIBO'],\n",
    "    obs_samples = subset_obs_data,\n",
    "    exploration_sets = exploration_sets,\n",
    "    priors_regret = bad_predicted_optima_regret,\n",
    "    total_timesteps = T,\n",
    "    number_of_trials = N,\n",
    "    reps = R, # Number of replicates (how many times we run each method)\n",
    "    n_restart = 1,\n",
    "    save_data = False,\n",
    "    n_obs = 5, # The method samples 5 time-series for each replicate\n",
    "    num_anchor_points = 100,\n",
    "    sample_anchor_points = True,\n",
    "    controlled_experiment=False,\n",
    "    args_sem=[functions_0, functions_t],\n",
    "    manipulative_variables=[\"P\", \"I\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = {\n",
    "    \"linewidth\": 3,\n",
    "    \"linewidth_opt\": 4,\n",
    "    \"alpha\": 0.2,\n",
    "    \"xlim_max\": N,\n",
    "    \"ncols\": 5,\n",
    "    \"loc_legend\": \"lower right\",\n",
    "    \"size_ticks\": 20,\n",
    "    \"size_labels\": 20,\n",
    "    \"xlabel\": r'$\\texttt{cost}(\\mathbf{X}_{s,t}, \\mathbf{x}_{s,t})$',\n",
    "    \"labels\": {'DCBO': 'DCBO', 'DCPIBO': 'DCPIBO', 'PIBO': 'PIBO', 'BO': 'BO', 'True': r'$\\mathbb{E} \\left [Y_t \\mid \\textrm{do}(\\mathbf{X}_{s,t}^\\star = \\mathbf{x}_{s,t}^\\star) \\right]$'},\n",
    "    \"colors\": {'DCBO': 'blue', 'DCPIBO': 'green', 'PIBO': 'orange', 'BO': 'red', 'True': 'black'},\n",
    "    \"line_styles\": {'DCBO': '-', 'DCPIBO': '--', 'PIBO': 'dashdot', 'BO': '-', 'True': ':'},\n",
    "    \"width\":10\n",
    "}\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex', preamble=r'\\usepackage{amssymb}')\n",
    "rc('font', family='serif')\n",
    "rc('font', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainOffsetScalarFormatter(ScalarFormatter):\n",
    "    def get_offset(self):\n",
    "        if len(self.locs) == 0:\n",
    "            return ''\n",
    "        if self.orderOfMagnitude:\n",
    "            pass\n",
    "            # print(\"Your plot will likely be labelled incorrectly\")\n",
    "        return \"$\\\\times 10^{-5}+$\" + str(round(self.offset, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we didn't save the results we cannot use the pickled file so we have to convert results to the correct format\n",
    "good_data = get_relevant_results(results=good_results, replicates=R)\n",
    "good_exp_optimal_outcome_values_during_trials, good_exp_per_trial_cost = elaborate(number_of_interventions=None, \n",
    "                                                                         n_replicates=R, \n",
    "                                                                         data=good_data, \n",
    "                                                                         best_objective_values=best_objective_values, \n",
    "                                                                         T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each frame corresponds to one time-slice.\n",
    "plot_expected_opt_curve_paper(T,\n",
    "    best_objective_values,\n",
    "    good_exp_per_trial_cost,\n",
    "    good_exp_optimal_outcome_values_during_trials,\n",
    "    plot_params, \n",
    "    fig_size = (15,5),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data = get_relevant_results(results=bad_results,replicates=R)\n",
    "bad_exp_optimal_outcome_values_during_trials, bad_exp_per_trial_cost = elaborate(number_of_interventions=None, \n",
    "                                                                         n_replicates=R, \n",
    "                                                                         data=bad_data, \n",
    "                                                                         best_objective_values=best_objective_values, \n",
    "                                                                         T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each frame corresponds to one time-slice.\n",
    "plot_expected_opt_curve_paper(T,\n",
    "    best_objective_values,\n",
    "    bad_exp_per_trial_cost,\n",
    "    bad_exp_optimal_outcome_values_during_trials,\n",
    "    plot_params, \n",
    "    fig_size = (15,5),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap_values(T, summary):\n",
    "    G_BO=0\n",
    "    G_PIBO=0\n",
    "    G_DCBO=0\n",
    "    G_DCPIBO=0\n",
    "    list_gaps=[]\n",
    "    summary_list = list(summary.values())\n",
    "    for i in range(len(summary_list)):\n",
    "        for t in range(T):\n",
    "            # print(summary_list[i][t][0])\n",
    "            if i==0:\n",
    "                G_BO=G_BO+summary_list[i][t][0]\n",
    "                if t==2:\n",
    "                    list_gaps.append(G_BO/T)\n",
    "            elif i==1:\n",
    "                G_PIBO=G_PIBO+summary_list[i][t][0]\n",
    "                if t==2:\n",
    "                    list_gaps.append(G_PIBO/T)\n",
    "            elif i==2:\n",
    "                G_DCBO=G_DCBO+summary_list[i][t][0]\n",
    "                if t==2:\n",
    "                    list_gaps.append(G_DCBO/T)\n",
    "            elif i==3:\n",
    "                G_DCPIBO=G_DCPIBO+summary_list[i][t][0]\n",
    "                if t==2:\n",
    "                    list_gaps.append(G_DCPIBO/T)\n",
    "    return list_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Gap Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_t_i_l = get_common_initial_values(T=T,\n",
    "                                  data=good_data,\n",
    "                                  n_replicates=R)\n",
    "\n",
    "good_w_c_d = get_converge_trial(best_objective_values=best_objective_values,\n",
    "                           exp_optimal_outcome_values_during_trials=good_exp_optimal_outcome_values_during_trials,\n",
    "                           n_trials=N,\n",
    "                           T=T,\n",
    "                           n_decimal=1)\n",
    "\n",
    "good_summary = gap_metric_standard(T=T,\n",
    "                             data=good_data,\n",
    "                             best_objective_values=best_objective_values,\n",
    "                             total_initial_list = good_t_i_l,\n",
    "                             n_replicates=R,\n",
    "                             n_trials=N,\n",
    "                             where_converge_dict=good_w_c_d)\n",
    "\n",
    "good_list = compute_gap_values(T, good_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Gap Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_t_i_l = get_common_initial_values(T=T,\n",
    "                                  data=bad_data,\n",
    "                                  n_replicates=R)\n",
    "\n",
    "bad_w_c_d = get_converge_trial(best_objective_values=best_objective_values,\n",
    "                          exp_optimal_outcome_values_during_trials=bad_exp_optimal_outcome_values_during_trials,\n",
    "                          n_trials=N,\n",
    "                          T=T,\n",
    "                          n_decimal=1)\n",
    "\n",
    "bad_summary = gap_metric_standard(T=T,\n",
    "                             data=bad_data,\n",
    "                             best_objective_values=best_objective_values,\n",
    "                             total_initial_list = bad_t_i_l,\n",
    "                             n_replicates=R,\n",
    "                             n_trials=N,\n",
    "                             where_converge_dict=bad_w_c_d)\n",
    "\n",
    "bad_list = compute_gap_values(T, bad_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_list"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e54279b2d173705d392bde41516bb8c915f7eee4cde8e160cde6d5088aa4048d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
